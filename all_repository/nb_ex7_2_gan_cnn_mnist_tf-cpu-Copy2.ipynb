{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 필기체를 생성하는 합성곱 계층 GAN 구현 \n",
    "GAN을 이용해 필기체 숫자를 생성하는 인공신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to use CPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 공통 패키지 불러오기\n",
    "- 공통 패키지 MNIST 데이터셋을 불러오는 케라스 서브패키지를 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "# The Conv2D op currently only supports the NHWC tensor format on the CPU\n",
    "import keras.backend as K\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 사용자 정의 손실 함수 만들기\n",
    "- 케라스가 제공하지 않는 함수를 사용자 정의로 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3 합성곱 계층 GAN 모델링\n",
    "- GAN에 포함된 2가지 신경망인 생성망과 판별망을 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=32): # input_dim = args.n_train = 32\n",
    "        \"\"\"\n",
    "        self, self.generator, self.discriminator are all models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "\n",
    "    def compile_all(self):\n",
    "        # Compiling stage\n",
    "        d_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer=\"SGD\")\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
    "        model.add(layers.Dense(7 * 7 * 128, activation='tanh')) # H, W, C = 7, 7, 128\n",
    "        model.add(layers.BatchNormalization())\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU.\n",
    "        model.add(layers.Reshape((7, 7, 128), input_shape=(7 * 7 * 128,)))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "        return model\n",
    "\n",
    "    def DISCRIMINATOR(self):\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU. \n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
    "                                input_shape=(28, 28, 1)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = np.array([1] * ln + [0] * ln).reshape(-1,1) # Necessary!\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        # Second trial for training generator\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, np.array([1] * ln).reshape(-1, 1))\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.4 합성곱 계층 GAN 학습 시키기\n",
    "- 앞서 만든 GAN의 모델링과 학습용 클래스를 사용해서 실제로 GAN을 학습하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0],\n",
    "        j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('Output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    # The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW\n",
    "    # X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:]) # <-- NCHW format \n",
    "    X_train = X_train.reshape(X_train.shape + (1,)) # <-- NHWC format\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch is\", epoch)\n",
    "            print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "\n",
    "    gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.5 합성곱 계층 GAN 수행 \n",
    "- 각 단계별 구현에 앞서 합성곱 계층 GAN의 수행 방법과 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output_fold is GAN_OUT\n",
      "Epoch is 0\n",
      "Number of batches 2\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    # Not implemented in Notebook\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "        help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "        help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT',\n",
    "        help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10,\n",
    "        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_train', type=int, default=32,\n",
    "        help='The number of training data.')\n",
    "    \"\"\"\n",
    "    class ARGS:\n",
    "        def __init__(args):\n",
    "            args.batch_size = 16\n",
    "            args.epochs = 10\n",
    "            args.output_fold = 'GAN_OUT'\n",
    "            args.input_dim = 10\n",
    "            args.n_train = 32\n",
    "\n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.2.6 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function image_data_format at 0x7f3e2b6c0ee0>\n",
      "Output_fold is GAN_OUT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_5/Identity)' with input shapes: [?,64,28,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1653\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_5/Identity)' with input shapes: [?,64,28,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32074/74795499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_32074/74795499.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32074/74795499.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <-- NHWC format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0md_loss_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32074/74795499.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGENERATOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISCRIMINATOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32074/74795499.py\u001b[0m in \u001b[0;36mDISCRIMINATOR\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n\u001b[1;32m     62\u001b[0m                                 input_shape=(28, 28, 1)))\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    921\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    289\u001b[0m       \u001b[0mpool_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m       \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     outputs = self.pool_function(\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpool_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name, input)\u001b[0m\n\u001b[1;32m   3921\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ksize cannot be zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m     return gen_nn_ops.max_pool(\n\u001b[0m\u001b[1;32m   3924\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   5232\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5233\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5234\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   5235\u001b[0m         \u001b[0;34m\"MaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5236\u001b[0m                    data_format=data_format, name=name)\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         compute_device)\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3317\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3319\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3320\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3321\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1814\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[1;32m   1817\u001b[0m                                 control_input_ops, op_def)\n\u001b[1;32m   1818\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"VALID\", strides=[1, 1, 2, 2]](conv2d_5/Identity)' with input shapes: [?,64,28,1]."
     ]
    }
   ],
   "source": [
    "################################\n",
    "# 공통 패키지 불러오기\n",
    "################################\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "from keras.datasets import mnist\n",
    "import keras.backend as K\n",
    "\n",
    "print(K.image_data_format)\n",
    "\n",
    "################################\n",
    "# GAN 모델링\n",
    "################################\n",
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=32): # input_dim = args.n_train = 32\n",
    "        \"\"\"\n",
    "        self, self.generator, self.discriminator are all models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "\n",
    "    def compile_all(self):\n",
    "        # Compiling stage\n",
    "        d_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer=\"SGD\")\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
    "        model.add(layers.Dense(7 * 7 * 128, activation='tanh')) # H, W, C = 7, 7, 128\n",
    "        model.add(layers.BatchNormalization())\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU.\n",
    "        model.add(layers.Reshape((7, 7, 128), input_shape=(7 * 7 * 128,)))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "        return model\n",
    "\n",
    "    def DISCRIMINATOR(self):\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU. \n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
    "                                input_shape=(28, 28, 1)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = np.array([1] * ln + [0] * ln).reshape(-1,1) # Necessary!\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        # Second trial for training generator\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, np.array([1] * ln).reshape(-1, 1))\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss\n",
    "\n",
    "################################\n",
    "# GAN 학습하기\n",
    "################################\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0],\n",
    "        j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('Output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    # The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW\n",
    "    # X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:]) # <-- NCHW format \n",
    "    X_train = X_train.reshape(X_train.shape + (1,)) # <-- NHWC format\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch is\", epoch)\n",
    "            print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "\n",
    "    gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)\n",
    "\n",
    "\n",
    "################################\n",
    "# GAN 예제 실행하기\n",
    "################################\n",
    "# import argparse\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    # Not implemented in Notebook\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "        help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "        help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT',\n",
    "        help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10,\n",
    "        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_train', type=int, default=32,\n",
    "        help='The number of training data.')\n",
    "    \"\"\"\n",
    "    class ARGS:\n",
    "        def __init__(args):\n",
    "            args.batch_size = 16\n",
    "            args.epochs = 1000\n",
    "            args.output_fold = 'GAN_OUT'\n",
    "            args.input_dim = 10\n",
    "            args.n_train = 32\n",
    "\n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
