{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 패키지\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 강화학습 환경 패키지\n",
    "import gym\n",
    "\n",
    "# 인공지능 패키지: 텐서플로, 케라스 \n",
    "# 호환성을 위해 텐스플로에 포함된 케라스를 불러옴 \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 722\n",
      "Trainable params: 722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_q_model(num_states, num_actions):\n",
    "    inputs = Input(shape=(num_states,))\n",
    "    layer = Dense(32, activation=\"relu\")(inputs)\n",
    "    layer = Dense(16, activation=\"relu\")(layer)\n",
    "    action = Dense(num_actions, activation=\"linear\")(layer)\n",
    "    return Model(inputs=inputs, outputs=action)\n",
    "\n",
    "model = create_q_model(4,2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple training is completed!\n"
     ]
    }
   ],
   "source": [
    "class World_00:\n",
    "    def __init__(self):\n",
    "        self.get_env_model()\n",
    "\n",
    "    def get_env_model(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.model = create_q_model(self.num_states, self.num_actions)\n",
    "        # print(self.model.summary())\n",
    "\n",
    "    def train(self):        \n",
    "        states = np.zeros((10,self.num_states), dtype=np.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicts = self.model(states)\n",
    "\n",
    "new_world = World_00()\n",
    "new_world.train()\n",
    "print('Simple training is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     0 -->  Score: 10.0\n",
      "Episode:     1 -->  Score: 10.0\n",
      "Episode:     2 -->  Score: 10.0\n",
      "Episode:     3 -->  Score: 10.0\n",
      "Episode:     4 -->  Score: 8.0\n",
      "Episode:     5 -->  Score: 8.0\n",
      "Episode:     6 -->  Score: 10.0\n",
      "Episode:     7 -->  Score: 9.0\n",
      "Episode:     8 -->  Score: 10.0\n",
      "Episode:     9 -->  Score: 9.0\n",
      "Notice that the max score is set to 500.0 in CartPole-v1\n",
      "94\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "def env_test_model_memory(memory, env, model, n_episodes=1000, \n",
    "        flag_render=False):\n",
    "    for e in range(n_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            s_array = np.array(s).reshape((1,-1))\n",
    "            Qsa = model.predict(s_array)[0]\n",
    "            a = np.argmax(Qsa)\n",
    "            next_s, r, done, _ = env.step(a)\n",
    "            if flag_render:\n",
    "                env.render()\n",
    "            score += r\n",
    "            memory.append([s,a,r,next_s,done])\n",
    "        print(f'Episode: {e:5d} -->  Score: {score:3.1f}')\n",
    "    print('Notice that the max score is set to 500.0 in CartPole-v1')\n",
    "\n",
    "def list_rotate(l):\n",
    "    return list(zip(*l))\n",
    "\n",
    "class World_01(World_00):\n",
    "    def __init__(self):\n",
    "        World_00.__init__(self)\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.N_batch = 64\n",
    "        self.t_model = create_q_model(self.num_states, self.num_actions)\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "\n",
    "    def trial(self, flag_render=False):\n",
    "        env_test_model_memory(self.memory, self.env,\n",
    "            self.model, n_episodes=10, flag_render=flag_render)\n",
    "        print(len(self.memory))\n",
    "\n",
    "    def train_memory(self):\n",
    "        if len(self.memory) >= self.N_batch:\n",
    "            memory_batch = random.sample(self.memory, self.N_batch)\n",
    "            s_l,a_l,r_l,next_s_l,done_l = [np.array(x) for x in list_rotate(memory_batch)]\n",
    "            model_w = self.model.trainable_variables\n",
    "            with tf.GradientTape() as tape:\n",
    "                Qsa_pred_l = self.model(s_l.astype(np.float32))\n",
    "                a_l_onehot = tf.one_hot(a_l, self.num_actions)\n",
    "                Qs_a_pred_l = tf.reduce_sum(a_l_onehot * Qsa_pred_l, axis=1)    \n",
    "\n",
    "                Qsa_tpred_l = self.t_model(next_s_l.astype(np.float32)) \n",
    "                Qsa_tpred_l = tf.stop_gradient(Qsa_tpred_l)\n",
    "\n",
    "                max_Q_next_s_a_l = np.amax(Qsa_tpred_l, axis=-1)\n",
    "                Qs_a_l = r_l + (1 - done_l) * self.discount_factor * max_Q_next_s_a_l\n",
    "                loss = tf.reduce_mean(tf.square(Qs_a_l - Qs_a_pred_l))\n",
    "            grads = tape.gradient(loss, model_w)\n",
    "            self.optimizer.apply_gradients(zip(grads, model_w))\n",
    "\n",
    "new_world = World_01()\n",
    "new_world.trial()\n",
    "new_world.train_memory()\n",
    "new_world.env.close()\n",
    "print('Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     0 -->  Score: 12.0\n",
      "Episode:     1 -->  Score: 10.0\n",
      "Episode:     2 -->  Score: 12.0\n",
      "Episode:     3 -->  Score: 12.0\n",
      "Episode:     4 -->  Score: 9.0\n",
      "Episode:     5 -->  Score: 10.0\n",
      "Episode:     6 -->  Score: 9.0\n",
      "Episode:     7 -->  Score: 9.0\n",
      "Episode:     8 -->  Score: 10.0\n",
      "Episode:     9 -->  Score: 11.0\n",
      "Episode:    10 -->  Score: 11.0\n",
      "Episode:    11 -->  Score: 10.0\n",
      "Episode:    12 -->  Score: 10.0\n",
      "Episode:    13 -->  Score: 10.0\n",
      "Episode:    14 -->  Score: 11.0\n",
      "Episode:    15 -->  Score: 12.0\n",
      "Episode:    16 -->  Score: 9.0\n",
      "Episode:    17 -->  Score: 10.0\n",
      "Episode:    18 -->  Score: 9.0\n",
      "Episode:    19 -->  Score: 10.0\n",
      "Episode:    20 -->  Score: 14.0\n",
      "Episode:    21 -->  Score: 11.0\n",
      "Episode:    22 -->  Score: 10.0\n",
      "Episode:    23 -->  Score: 11.0\n",
      "Episode:    24 -->  Score: 13.0\n",
      "Episode:    25 -->  Score: 11.0\n",
      "Episode:    26 -->  Score: 9.0\n",
      "Episode:    27 -->  Score: 10.0\n",
      "Episode:    28 -->  Score: 10.0\n",
      "Episode:    29 -->  Score: 11.0\n",
      "Episode:    30 -->  Score: 10.0\n",
      "Episode:    31 -->  Score: 16.0\n",
      "Episode:    32 -->  Score: 11.0\n",
      "Episode:    33 -->  Score: 10.0\n",
      "Episode:    34 -->  Score: 10.0\n",
      "Episode:    35 -->  Score: 12.0\n",
      "Episode:    36 -->  Score: 13.0\n",
      "Episode:    37 -->  Score: 13.0\n",
      "Episode:    38 -->  Score: 12.0\n",
      "Episode:    39 -->  Score: 11.0\n",
      "Episode:    40 -->  Score: 11.0\n",
      "Episode:    41 -->  Score: 11.0\n",
      "Episode:    42 -->  Score: 12.0\n",
      "Episode:    43 -->  Score: 10.0\n",
      "Episode:    44 -->  Score: 13.0\n",
      "Episode:    45 -->  Score: 16.0\n",
      "Episode:    46 -->  Score: 71.0\n",
      "Episode:    47 -->  Score: 57.0\n",
      "Episode:    48 -->  Score: 11.0\n",
      "Episode:    49 -->  Score: 99.0\n"
     ]
    }
   ],
   "source": [
    "class World_02(World_01):\n",
    "    def __init__(self):\n",
    "        World_01.__init__(self)\n",
    "        self.epsilon = 0.2\n",
    "    \n",
    "    def update_t_model(self):\n",
    "        self.t_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def best_action(self, s):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            s_array = np.array(s).reshape((1,-1))\n",
    "            Qsa = self.model.predict(s_array)[0]\n",
    "            return np.argmax(Qsa)\n",
    "\n",
    "    def trials(self, n_episodes=100, flag_render=False):\n",
    "        memory = self.memory\n",
    "        env = self.env\n",
    "        model = self.model\n",
    "        score_l = []\n",
    "        for e in range(n_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            s = env.reset()\n",
    "            while not done:                \n",
    "                a = self.best_action(s)\n",
    "                next_s, r, done, _ = env.step(a)\n",
    "                if flag_render:\n",
    "                    env.render()\n",
    "                score += r\n",
    "                memory.append([s,a,r,next_s,done])\n",
    "                # self.train_memory()     \n",
    "                s = next_s\n",
    "                self.train_memory()                 \n",
    "            self.update_t_model()\n",
    "            print(f'Episode: {e:5d} -->  Score: {score:3.1f}') \n",
    "            score_l.append(score)            \n",
    "        return score_l\n",
    "\n",
    "new_world = World_02()\n",
    "score_l = new_world.trials(n_episodes=50)\n",
    "new_world.env.close()\n",
    "np.save('score_l.npy', score_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Codes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
