{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:    0/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    1/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    2/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    3/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    4/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    5/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    6/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    7/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    8/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:    9/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:   10/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:   11/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:   12/4000 and step:   19. Eps: 1.0, reward 0.0\n",
      "Episode:   13/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:   14/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:   15/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:   16/4000 and step:   99. Eps: 1.0, reward 0.0\n",
      "Episode:   17/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   18/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   19/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   20/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   21/4000 and step:   57. Eps: 0.99, reward 0.0\n",
      "Episode:   22/4000 and step:    7. Eps: 0.99, reward 0.0\n",
      "Episode:   23/4000 and step:   51. Eps: 0.99, reward 0.0\n",
      "Episode:   24/4000 and step:   11. Eps: 0.99, reward 0.0\n",
      "Episode:   25/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   26/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   27/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   28/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   29/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   30/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   31/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   32/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   33/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   34/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   35/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   36/4000 and step:   99. Eps: 0.99, reward 0.0\n",
      "Episode:   37/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   38/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   39/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   40/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   41/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   42/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   43/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   44/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   45/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   46/4000 and step:    1. Eps: 0.99, reward 0.0\n",
      "Episode:   47/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   48/4000 and step:   99. Eps: 0.98, reward 0.0\n",
      "Episode:   49/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   50/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   51/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   52/4000 and step:    2. Eps: 0.98, reward 0.0\n",
      "Episode:   53/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   54/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   55/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   56/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   57/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   58/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   59/4000 and step:   99. Eps: 0.98, reward 0.0\n",
      "Episode:   60/4000 and step:   77. Eps: 0.98, reward 0.0\n",
      "Episode:   61/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   62/4000 and step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   63/4000 and step:   67. Eps: 0.98, reward 0.0\n",
      "Episode:   64/4000 and step:   22. Eps: 0.98, reward 0.0\n",
      "Episode:   65/4000 and step:    2. Eps: 0.98, reward 0.0\n",
      "Episode:   66/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   67/4000 and step:   99. Eps: 0.98, reward 0.0\n",
      "Episode:   68/4000 and step:    1. Eps: 0.98, reward 0.0\n",
      "Episode:   69/4000 and step:   65. Eps: 0.98, reward 0.0\n",
      "Episode:   70/4000 and step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   71/4000 and step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   72/4000 and step:   99. Eps: 0.98, reward 0.0\n",
      "Episode:   73/4000 and step:   70. Eps: 0.98, reward 0.0\n",
      "Episode:   74/4000 and step:   99. Eps: 0.98, reward 0.0\n",
      "Episode:   75/4000 and step:    3. Eps: 0.98, reward 0.0\n",
      "Episode:   76/4000 and step:   99. Eps: 0.98, reward 0.0\n",
      "Episode:   77/4000 and step:   62. Eps: 0.98, reward 0.0\n",
      "Episode:   78/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   79/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   80/4000 and step:   70. Eps: 0.97, reward 0.0\n",
      "Episode:   81/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:   82/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:   83/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   84/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   85/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   86/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   87/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   88/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   89/4000 and step:    5. Eps: 0.97, reward 0.0\n",
      "Episode:   90/4000 and step:    3. Eps: 0.97, reward 0.0\n",
      "Episode:   91/4000 and step:    4. Eps: 0.97, reward 0.0\n",
      "Episode:   92/4000 and step:   29. Eps: 0.97, reward 0.0\n",
      "Episode:   93/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:   94/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:   95/4000 and step:   57. Eps: 0.97, reward 0.0\n",
      "Episode:   96/4000 and step:   20. Eps: 0.97, reward 0.0\n",
      "Episode:   97/4000 and step:   93. Eps: 0.97, reward 0.0\n",
      "Episode:   98/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:   99/4000 and step:   15. Eps: 0.97, reward 0.0\n",
      "Episode:  100/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:  101/4000 and step:    1. Eps: 0.97, reward 0.0\n",
      "Episode:  102/4000 and step:   80. Eps: 0.97, reward 0.0\n",
      "Episode:  103/4000 and step:   67. Eps: 0.97, reward 0.0\n",
      "Episode:  104/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:  105/4000 and step:   15. Eps: 0.97, reward 0.0\n",
      "Episode:  106/4000 and step:   49. Eps: 0.97, reward 0.0\n",
      "Episode:  107/4000 and step:    5. Eps: 0.97, reward 0.0\n",
      "Episode:  108/4000 and step:   99. Eps: 0.97, reward 0.0\n",
      "Episode:  109/4000 and step:   34. Eps: 0.96, reward 0.0\n",
      "Episode:  110/4000 and step:   99. Eps: 0.96, reward 0.0\n",
      "Episode:  111/4000 and step:   99. Eps: 0.96, reward 0.0\n",
      "Episode:  112/4000 and step:   24. Eps: 0.96, reward 0.0\n",
      "Episode:  113/4000 and step:   73. Eps: 0.96, reward 0.0\n",
      "Episode:  114/4000 and step:    3. Eps: 0.96, reward 0.0\n",
      "Episode:  115/4000 and step:    3. Eps: 0.96, reward 0.0\n",
      "Episode:  116/4000 and step:    5. Eps: 0.96, reward 0.0\n",
      "Episode:  117/4000 and step:   11. Eps: 0.96, reward 0.0\n",
      "Episode:  118/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  119/4000 and step:    3. Eps: 0.96, reward 0.0\n",
      "Episode:  120/4000 and step:    3. Eps: 0.96, reward 0.0\n",
      "Episode:  121/4000 and step:    3. Eps: 0.96, reward 0.0\n",
      "Episode:  122/4000 and step:    3. Eps: 0.96, reward 0.0\n",
      "Episode:  123/4000 and step:    3. Eps: 0.96, reward 0.0\n",
      "Episode:  124/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  125/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  126/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  127/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  128/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  129/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  130/4000 and step:   99. Eps: 0.96, reward 0.0\n",
      "Episode:  131/4000 and step:   99. Eps: 0.96, reward 0.0\n",
      "Episode:  132/4000 and step:   10. Eps: 0.96, reward 0.0\n",
      "Episode:  133/4000 and step:    1. Eps: 0.96, reward 0.0\n",
      "Episode:  134/4000 and step:   18. Eps: 0.96, reward 0.0\n",
      "Episode:  135/4000 and step:   99. Eps: 0.96, reward 0.0\n",
      "Episode:  136/4000 and step:   60. Eps: 0.96, reward 0.0\n",
      "Episode:  137/4000 and step:   99. Eps: 0.96, reward 0.0\n",
      "Episode:  138/4000 and step:   99. Eps: 0.96, reward 0.0\n",
      "Episode:  139/4000 and step:   26. Eps: 0.96, reward 0.0\n",
      "Episode:  140/4000 and step:   90. Eps: 0.96, reward 0.0\n",
      "Episode:  141/4000 and step:    1. Eps: 0.95, reward 0.0\n",
      "Episode:  142/4000 and step:   92. Eps: 0.95, reward 0.0\n",
      "Episode:  143/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  144/4000 and step:    1. Eps: 0.95, reward 0.0\n",
      "Episode:  145/4000 and step:    9. Eps: 0.95, reward 0.0\n",
      "Episode:  146/4000 and step:   46. Eps: 0.95, reward 0.0\n",
      "Episode:  147/4000 and step:    1. Eps: 0.95, reward 0.0\n",
      "Episode:  148/4000 and step:   61. Eps: 0.95, reward 0.0\n",
      "Episode:  149/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  150/4000 and step:   51. Eps: 0.95, reward 0.0\n",
      "Episode:  151/4000 and step:   36. Eps: 0.95, reward 0.0\n",
      "Episode:  152/4000 and step:   30. Eps: 0.95, reward 0.0\n",
      "Episode:  153/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  154/4000 and step:   13. Eps: 0.95, reward 0.0\n",
      "Episode:  155/4000 and step:   40. Eps: 0.95, reward 0.0\n",
      "Episode:  156/4000 and step:   63. Eps: 0.95, reward 0.0\n",
      "Episode:  157/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  158/4000 and step:   49. Eps: 0.95, reward 1.0\n",
      "Episode:  159/4000 and step:   10. Eps: 0.95, reward 0.0\n",
      "Episode:  160/4000 and step:   92. Eps: 0.95, reward 0.0\n",
      "Episode:  161/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  162/4000 and step:   75. Eps: 0.95, reward 0.0\n",
      "Episode:  163/4000 and step:   23. Eps: 0.95, reward 0.0\n",
      "Episode:  164/4000 and step:   30. Eps: 0.95, reward 0.0\n",
      "Episode:  165/4000 and step:   83. Eps: 0.95, reward 1.0\n",
      "Episode:  166/4000 and step:   12. Eps: 0.95, reward 0.0\n",
      "Episode:  167/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  168/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  169/4000 and step:   73. Eps: 0.95, reward 0.0\n",
      "Episode:  170/4000 and step:   99. Eps: 0.95, reward 0.0\n",
      "Episode:  171/4000 and step:   37. Eps: 0.95, reward 0.0\n",
      "Episode:  172/4000 and step:    9. Eps: 0.95, reward 0.0\n",
      "Episode:  173/4000 and step:   37. Eps: 0.94, reward 0.0\n",
      "Episode:  174/4000 and step:   40. Eps: 0.94, reward 0.0\n",
      "Episode:  175/4000 and step:   12. Eps: 0.94, reward 0.0\n",
      "Episode:  176/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  177/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  178/4000 and step:   28. Eps: 0.94, reward 0.0\n",
      "Episode:  179/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  180/4000 and step:    3. Eps: 0.94, reward 0.0\n",
      "Episode:  181/4000 and step:   26. Eps: 0.94, reward 0.0\n",
      "Episode:  182/4000 and step:   49. Eps: 0.94, reward 0.0\n",
      "Episode:  183/4000 and step:    3. Eps: 0.94, reward 0.0\n",
      "Episode:  184/4000 and step:    2. Eps: 0.94, reward 0.0\n",
      "Episode:  185/4000 and step:   11. Eps: 0.94, reward 0.0\n",
      "Episode:  186/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  187/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  188/4000 and step:   27. Eps: 0.94, reward 0.0\n",
      "Episode:  189/4000 and step:   30. Eps: 0.94, reward 0.0\n",
      "Episode:  190/4000 and step:    2. Eps: 0.94, reward 0.0\n",
      "Episode:  191/4000 and step:   25. Eps: 0.94, reward 1.0\n",
      "Episode:  192/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  193/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  194/4000 and step:    3. Eps: 0.94, reward 0.0\n",
      "Episode:  195/4000 and step:    4. Eps: 0.94, reward 0.0\n",
      "Episode:  196/4000 and step:    3. Eps: 0.94, reward 0.0\n",
      "Episode:  197/4000 and step:    2. Eps: 0.94, reward 0.0\n",
      "Episode:  198/4000 and step:    1. Eps: 0.94, reward 0.0\n",
      "Episode:  199/4000 and step:    5. Eps: 0.94, reward 0.0\n",
      "Episode:  200/4000 and step:    3. Eps: 0.94, reward 0.0\n",
      "Episode:  201/4000 and step:   16. Eps: 0.94, reward 0.0\n",
      "Episode:  202/4000 and step:    3. Eps: 0.94, reward 0.0\n",
      "Episode:  203/4000 and step:   99. Eps: 0.94, reward 0.0\n",
      "Episode:  204/4000 and step:   44. Eps: 0.94, reward 1.0\n",
      "Episode:  205/4000 and step:   24. Eps: 0.93, reward 0.0\n",
      "Episode:  206/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  207/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  208/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  209/4000 and step:   32. Eps: 0.93, reward 0.0\n",
      "Episode:  210/4000 and step:   32. Eps: 0.93, reward 0.0\n",
      "Episode:  211/4000 and step:    1. Eps: 0.93, reward 0.0\n",
      "Episode:  212/4000 and step:   64. Eps: 0.93, reward 0.0\n",
      "Episode:  213/4000 and step:    1. Eps: 0.93, reward 0.0\n",
      "Episode:  214/4000 and step:   51. Eps: 0.93, reward 0.0\n",
      "Episode:  215/4000 and step:    2. Eps: 0.93, reward 0.0\n",
      "Episode:  216/4000 and step:   99. Eps: 0.93, reward 0.0\n",
      "Episode:  217/4000 and step:   65. Eps: 0.93, reward 0.0\n",
      "Episode:  218/4000 and step:    4. Eps: 0.93, reward 0.0\n",
      "Episode:  219/4000 and step:    1. Eps: 0.93, reward 0.0\n",
      "Episode:  220/4000 and step:   12. Eps: 0.93, reward 0.0\n",
      "Episode:  221/4000 and step:    1. Eps: 0.93, reward 0.0\n",
      "Episode:  222/4000 and step:    1. Eps: 0.93, reward 0.0\n",
      "Episode:  223/4000 and step:   18. Eps: 0.93, reward 0.0\n",
      "Episode:  224/4000 and step:    5. Eps: 0.93, reward 0.0\n",
      "Episode:  225/4000 and step:   10. Eps: 0.93, reward 0.0\n",
      "Episode:  226/4000 and step:    5. Eps: 0.93, reward 1.0\n",
      "Episode:  227/4000 and step:   44. Eps: 0.93, reward 0.0\n",
      "Episode:  228/4000 and step:   62. Eps: 0.93, reward 0.0\n",
      "Episode:  229/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  230/4000 and step:   99. Eps: 0.93, reward 0.0\n",
      "Episode:  231/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  232/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  233/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  234/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  235/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  236/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  237/4000 and step:    3. Eps: 0.93, reward 0.0\n",
      "Episode:  238/4000 and step:   16. Eps: 0.92, reward 0.0\n",
      "Episode:  239/4000 and step:   84. Eps: 0.92, reward 0.0\n",
      "Episode:  240/4000 and step:    5. Eps: 0.92, reward 0.0\n",
      "Episode:  241/4000 and step:   99. Eps: 0.92, reward 0.0\n",
      "Episode:  242/4000 and step:    1. Eps: 0.92, reward 0.0\n",
      "Episode:  243/4000 and step:    7. Eps: 0.92, reward 0.0\n",
      "Episode:  244/4000 and step:    4. Eps: 0.92, reward 0.0\n",
      "Episode:  245/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  246/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  247/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  248/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  249/4000 and step:   22. Eps: 0.92, reward 0.0\n",
      "Episode:  250/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  251/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  252/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  253/4000 and step:   58. Eps: 0.92, reward 0.0\n",
      "Episode:  254/4000 and step:   15. Eps: 0.92, reward 1.0\n",
      "Episode:  255/4000 and step:   15. Eps: 0.92, reward 0.0\n",
      "Episode:  256/4000 and step:    5. Eps: 0.92, reward 1.0\n",
      "Episode:  257/4000 and step:    5. Eps: 0.92, reward 1.0\n",
      "Episode:  258/4000 and step:    3. Eps: 0.92, reward 0.0\n",
      "Episode:  259/4000 and step:    5. Eps: 0.92, reward 1.0\n",
      "Episode:  260/4000 and step:   46. Eps: 0.92, reward 0.0\n",
      "Episode:  261/4000 and step:    4. Eps: 0.92, reward 0.0\n",
      "Episode:  262/4000 and step:   31. Eps: 0.92, reward 1.0\n",
      "Episode:  263/4000 and step:    3. Eps: 0.92, reward 0.0\n",
      "Episode:  264/4000 and step:    3. Eps: 0.92, reward 0.0\n",
      "Episode:  265/4000 and step:    3. Eps: 0.92, reward 0.0\n",
      "Episode:  266/4000 and step:    3. Eps: 0.92, reward 0.0\n",
      "Episode:  267/4000 and step:    1. Eps: 0.92, reward 0.0\n",
      "Episode:  268/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  269/4000 and step:    3. Eps: 0.92, reward 0.0\n",
      "Episode:  270/4000 and step:    2. Eps: 0.92, reward 0.0\n",
      "Episode:  271/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  272/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  273/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  274/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  275/4000 and step:   69. Eps: 0.91, reward 0.0\n",
      "Episode:  276/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  277/4000 and step:    1. Eps: 0.91, reward 0.0\n",
      "Episode:  278/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  279/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  280/4000 and step:   10. Eps: 0.91, reward 0.0\n",
      "Episode:  281/4000 and step:    1. Eps: 0.91, reward 0.0\n",
      "Episode:  282/4000 and step:    1. Eps: 0.91, reward 0.0\n",
      "Episode:  283/4000 and step:   14. Eps: 0.91, reward 0.0\n",
      "Episode:  284/4000 and step:    1. Eps: 0.91, reward 0.0\n",
      "Episode:  285/4000 and step:    1. Eps: 0.91, reward 0.0\n",
      "Episode:  286/4000 and step:   66. Eps: 0.91, reward 0.0\n",
      "Episode:  287/4000 and step:   99. Eps: 0.91, reward 0.0\n",
      "Episode:  288/4000 and step:   43. Eps: 0.91, reward 0.0\n",
      "Episode:  289/4000 and step:    9. Eps: 0.91, reward 0.0\n",
      "Episode:  290/4000 and step:    1. Eps: 0.91, reward 0.0\n",
      "Episode:  291/4000 and step:   11. Eps: 0.91, reward 0.0\n",
      "Episode:  292/4000 and step:    2. Eps: 0.91, reward 0.0\n",
      "Episode:  293/4000 and step:   16. Eps: 0.91, reward 0.0\n",
      "Episode:  294/4000 and step:    3. Eps: 0.91, reward 0.0\n",
      "Episode:  295/4000 and step:   14. Eps: 0.91, reward 1.0\n",
      "Episode:  296/4000 and step:   26. Eps: 0.91, reward 0.0\n",
      "Episode:  297/4000 and step:   22. Eps: 0.91, reward 0.0\n",
      "Episode:  298/4000 and step:    5. Eps: 0.91, reward 0.0\n",
      "Episode:  299/4000 and step:   45. Eps: 0.91, reward 0.0\n",
      "Episode:  300/4000 and step:   60. Eps: 0.91, reward 0.0\n",
      "Episode:  301/4000 and step:   32. Eps: 0.91, reward 0.0\n",
      "Episode:  302/4000 and step:   12. Eps: 0.91, reward 1.0\n",
      "Episode:  303/4000 and step:    7. Eps: 0.91, reward 0.0\n",
      "Episode:  304/4000 and step:    3. Eps: 0.9, reward 0.0\n",
      "Episode:  305/4000 and step:   13. Eps: 0.9, reward 0.0\n",
      "Episode:  306/4000 and step:    1. Eps: 0.9, reward 0.0\n",
      "Episode:  307/4000 and step:    6. Eps: 0.9, reward 1.0\n",
      "Episode:  308/4000 and step:   99. Eps: 0.9, reward 0.0\n",
      "Episode:  309/4000 and step:    4. Eps: 0.9, reward 0.0\n",
      "Episode:  310/4000 and step:    5. Eps: 0.9, reward 1.0\n",
      "Episode:  311/4000 and step:    7. Eps: 0.9, reward 1.0\n",
      "Episode:  312/4000 and step:    8. Eps: 0.9, reward 0.0\n",
      "Episode:  313/4000 and step:    2. Eps: 0.9, reward 0.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Q Learning / Frozen Lake / Not Slippery / 4x4\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from collections import deque\n",
    "\n",
    "custom_map = [\n",
    "    'FFFF',\n",
    "    'FHFH',\n",
    "    'FFFH',\n",
    "    'HFFG'\n",
    "]\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\", desc=custom_map, is_slippery=False)\n",
    "train_episodes=4000\n",
    "test_episodes=100\n",
    "max_steps=300\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "batch_size=32\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.memory = deque(maxlen=2500)\n",
    "        self.learning_rate=0.001\n",
    "        self.epsilon=1\n",
    "        self.max_eps=1\n",
    "        self.min_eps=0.01\n",
    "        self.eps_decay = 0.001/3\n",
    "        self.gamma=0.9\n",
    "        self.state_size= state_size\n",
    "        self.action_size= action_size\n",
    "        self.epsilon_lst=[]\n",
    "        self.model = self.buildmodel()\n",
    "\n",
    "    def buildmodel(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(10, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def add_memory(self, new_state, reward, done, state, action):\n",
    "        self.memory.append((new_state, reward, done, state, action))\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            return np.random.randint(0,4)\n",
    "        return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def pred(self, state):\n",
    "        return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self,batch_size):\n",
    "        minibatch=random.sample(self.memory, batch_size)\n",
    "        for new_state, reward, done, state, action in minibatch:\n",
    "            target= reward\n",
    "            if not done:\n",
    "                target=reward + self.gamma* np.amax(self.model.predict(new_state))\n",
    "            target_f= self.model.predict(state)\n",
    "            target_f[0][action]= target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.min_eps:\n",
    "            self.epsilon=(self.max_eps - self.min_eps) * np.exp(-self.eps_decay*episode) + self.min_eps\n",
    "\n",
    "        self.epsilon_lst.append(self.epsilon)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "agent=Agent(state_size, action_size)\n",
    "\n",
    "reward_lst=[]\n",
    "for episode in range(train_episodes):\n",
    "    state= env.reset()\n",
    "    state_arr=np.zeros(state_size)\n",
    "    state_arr[state] = 1\n",
    "    state= np.reshape(state_arr, [1, state_size])\n",
    "    reward = 0\n",
    "    done = False\n",
    "    for t in range(max_steps):\n",
    "        # env.render()\n",
    "        action = agent.action(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state_arr = np.zeros(state_size)\n",
    "        new_state_arr[new_state] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, state_size])\n",
    "        agent.add_memory(new_state, reward, done, state, action)\n",
    "        state= new_state\n",
    "\n",
    "        if done:\n",
    "            print(f'Episode: {episode:4}/{train_episodes} and step: {t:4}. Eps: {float(agent.epsilon):.2}, reward {reward}')\n",
    "            break\n",
    "\n",
    "    reward_lst.append(reward)\n",
    "\n",
    "    if len(agent.memory)> batch_size:\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "print(' Train mean % score= ', round(100*np.mean(reward_lst),1))\n",
    "\n",
    "# test\n",
    "test_wins=[]\n",
    "for episode in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    state_arr=np.zeros(state_size)\n",
    "    state_arr[state] = 1\n",
    "    state= np.reshape(state_arr, [1, state_size])\n",
    "    done = False\n",
    "    reward=0\n",
    "    state_lst = []\n",
    "    state_lst.append(state)\n",
    "    print('******* EPISODE ',episode, ' *******')\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.pred(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state_arr = np.zeros(state_size)\n",
    "        new_state_arr[new_state] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, state_size])\n",
    "        state = new_state\n",
    "        state_lst.append(state)\n",
    "        if done:\n",
    "            print(reward)\n",
    "            # env.render()\n",
    "            break\n",
    "\n",
    "    test_wins.append(reward)\n",
    "env.close()\n",
    "\n",
    "print(' Test mean % score= ', int(100*np.mean(test_wins)))\n",
    "\n",
    "fig=plt.figure(figsize=(10,12))\n",
    "matplotlib.rcParams.clear()\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "plt.subplot(311)\n",
    "plt.scatter(list(range(len(reward_lst))), reward_lst, s=0.2)\n",
    "plt.title('4x4 Frozen Lake Result(DQN) \\n \\nTrain Score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.scatter(list(range(len(agent.epsilon_lst))), agent.epsilon_lst, s=0.2)\n",
    "plt.title('Epsilon')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.scatter(list(range(len(test_wins))), test_wins, s=0.5)\n",
    "plt.title('Test Score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim((0,1.1))\n",
    "plt.savefig('result.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
