{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 필기체를 생성하는 합성곱 계층 GAN 구현 \n",
    "GAN을 이용해 필기체 숫자를 생성하는 인공신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to use CPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 공통 패키지 불러오기\n",
    "- 공통 패키지 MNIST 데이터셋을 불러오는 케라스 서브패키지를 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "# The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW\n",
    "import keras.backend as K\n",
    "print(K.image_data_format())\n",
    "# K.set_image_data_format('channels_first')\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 사용자 정의 손실 함수 만들기\n",
    "- 케라스가 제공하지 않는 함수를 사용자 정의로 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3 합성곱 계층 GAN 모델링\n",
    "- GAN에 포함된 2가지 신경망인 생성망과 판별망을 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=32): # input_dim = args.n_train = 32\n",
    "        \"\"\"\n",
    "        self, self.generator, self.discriminator are all models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "\n",
    "    def compile_all(self):\n",
    "        # Compiling stage\n",
    "        d_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer=\"SGD\")\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
    "        model.add(layers.Dense(7 * 7 * 128, activation='tanh')) # H, W, C = 7, 7, 128\n",
    "        model.add(layers.BatchNormalization())\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU.\n",
    "        model.add(layers.Reshape((7, 7, 128), input_shape=(7 * 7 * 128,)))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "        return model\n",
    "\n",
    "    def DISCRIMINATOR(self):\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU. \n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
    "                                input_shape=(28, 28, 1)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = [1] * ln + [0] * ln\n",
    "        y2 = np.array(y2).reshape(-1,1)\n",
    "        \n",
    "        #------------------\n",
    "        # ValueError: logits and labels must have the same shape ((32, 1) vs ())\n",
    "        # logits and labels must have the same shape ((16, 1) vs ())\n",
    "        # (32, 28, 28, 1) 32\n",
    "        print(xw.shape, y2.shape)\n",
    "        #------------------\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        # Second trial for training generator\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, [1] * ln)\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.4 합성곱 계층 GAN 학습 시키기\n",
    "- 앞서 만든 GAN의 모델링과 학습용 클래스를 사용해서 실제로 GAN을 학습하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# GAN 학습하기\n",
    "################################\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0],\n",
    "        j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('Output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    # The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW\n",
    "    # X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:]) # <-- NCHW format \n",
    "    X_train = X_train.reshape(X_train.shape + (1,)) # <-- NHWC format\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "\n",
    "    gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.5 합성곱 계층 GAN 수행 \n",
    "- 각 단계별 구현에 앞서 합성곱 계층 GAN의 수행 방법과 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output_fold is GAN_OUT\n",
      "Epoch is 0\n",
      "Number of batches 2\n",
      "(32, 28, 28, 1) (32, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1595 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4692 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:171 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((16, 1) vs ())\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10540/461260173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10540/461260173.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10540/2148547308.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0md_loss_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10540/3403776385.py\u001b[0m in \u001b[0;36mtrain_both\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                                     class_weight)\n\u001b[1;32m   1347\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1595 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4692 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/sjkim/anaconda3/envs/keras-gpu/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:171 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((16, 1) vs ())\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    # Not implemented in Notebook\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "        help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "        help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT',\n",
    "        help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10,\n",
    "        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_train', type=int, default=32,\n",
    "        help='The number of training data.')\n",
    "    \"\"\"\n",
    "    class ARGS:\n",
    "        def __init__(args):\n",
    "            args.batch_size = 16\n",
    "            args.epochs = 1000\n",
    "            args.output_fold = 'GAN_OUT'\n",
    "            args.input_dim = 10\n",
    "            args.n_train = 32\n",
    "\n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 코드 (임시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 32\n",
    "X_train = load_data(n_train)\n",
    "\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = X_train.reshape(X_train.shape + (1,))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1)\n",
      "(32, 28, 28, 64)\n",
      "(32, 14, 14, 64)\n",
      "(32, 10, 10, 128)\n",
      "(32, 5, 5, 128)\n",
      "(32, 3200)\n",
      "(32, 1024)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(32, 28, 28, 1)\n",
    "x = x.astype(np.float32)\n",
    "\n",
    "y = x\n",
    "print(y.shape)\n",
    "y = layers.Conv2D(64, (5, 5), padding='same', activation='tanh')(y)\n",
    "print(y.shape)\n",
    "y = layers.MaxPooling2D(pool_size=(2, 2))(y)\n",
    "print(y.shape)\n",
    "y = layers.Conv2D(128, (5, 5), activation='tanh')(y)\n",
    "print(y.shape)\n",
    "y = layers.MaxPooling2D(pool_size=(2, 2))(y)\n",
    "print(y.shape)\n",
    "y = layers.Flatten()(y)\n",
    "print(y.shape)\n",
    "y = layers.Dense(1024, activation='tanh')(y)\n",
    "print(y.shape)\n",
    "y = layers.Dense(1, activation='sigmoid')(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.2.6 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# 공통 패키지 불러오기\n",
    "################################\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "from keras.datasets import mnist\n",
    "import keras.backend as K\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "print(K.image_data_format)\n",
    "\n",
    "################################\n",
    "# GAN 모델링\n",
    "################################\n",
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=64):\n",
    "        \"\"\"\n",
    "        self, self.generator, self.discriminator are all models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "\n",
    "    def compile_all(self):\n",
    "        # Compiling stage\n",
    "        d_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer=\"SGD\")\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
    "        model.add(layers.Dense(128 * 7 * 7, activation='tanh'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "        return model\n",
    "\n",
    "    def DISCRIMINATOR(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
    "                                input_shape=(1, 28, 28)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = [1] * ln + [0] * ln\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        # Second trial for training generator\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, [1] * ln)\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss\n",
    "\n",
    "################################\n",
    "# GAN 학습하기\n",
    "################################\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0],\n",
    "        j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('Output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "\n",
    "    gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)\n",
    "\n",
    "\n",
    "################################\n",
    "# GAN 예제 실행하기\n",
    "################################\n",
    "# import argparse\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    # Not implemented in Notebook\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "        help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "        help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT',\n",
    "        help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10,\n",
    "        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_train', type=int, default=32,\n",
    "        help='The number of training data.')\n",
    "    \"\"\"\n",
    "    class ARGS:\n",
    "        def __init__(args):\n",
    "            args.batch_size = 16\n",
    "            args.epochs = 1000\n",
    "            args.output_fold = 'GAN_OUT'\n",
    "            args.input_dim = 10\n",
    "            args.n_train = 32\n",
    "\n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
