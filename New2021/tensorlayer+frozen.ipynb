{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train] [--test]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/sjkim/.local/share/jupyter/runtime/kernel-df9123b8-c26e-432c-9aa2-0875347f6c13.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjkim/anaconda3/envs/keras/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Q-Network Q(a, s)\n",
    "-----------------------\n",
    "TD Learning, Off-Policy, e-Greedy Exploration (GLIE).\n",
    "Q(S, A) <- Q(S, A) + alpha * (R + lambda * Q(newS, newA) - Q(S, A))\n",
    "delta_w = R + lambda * Q(newS, newA)\n",
    "See David Silver RL Tutorial Lecture 5 - Q-Learning for more details.\n",
    "Reference\n",
    "----------\n",
    "original paper: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "EN: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.5m3361vlw\n",
    "CN: https://zhuanlan.zhihu.com/p/25710327\n",
    "Note: Policy Network has been proved to be better than Q-Learning, see tutorial_atari_pong.py\n",
    "Environment\n",
    "-----------\n",
    "# The FrozenLake v0 environment\n",
    "https://gym.openai.com/envs/FrozenLake-v0\n",
    "The agent controls the movement of a character in a grid world. Some tiles of\n",
    "the grid are walkable, and others lead to the agent falling into the water.\n",
    "Additionally, the movement direction of the agent is uncertain and only partially\n",
    "depends on the chosen direction. The agent is rewarded for finding a walkable\n",
    "path to a goal tile.\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward\n",
    "of 1 if you reach the goal, and zero otherwise.\n",
    "Prerequisites\n",
    "--------------\n",
    "tensorflow>=2.0.0a0\n",
    "tensorlayer>=2.0.0\n",
    "To run\n",
    "-------\n",
    "python tutorial_DQN.py --train/test\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "# add arguments in command  --train/test\n",
    "parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "parser.add_argument('--train', dest='train', action='store_true', default=True)\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "args = parser.parse_args()\n",
    "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "env_id = 'FrozenLake-v0'\n",
    "alg_name = 'DQN'\n",
    "lambd = .99  # decay factor\n",
    "e = 0.1  # e-Greedy Exploration, the larger the more random\n",
    "num_episodes = 10000\n",
    "render = False  # display the game environment\n",
    "rList = [] #Record reward\n",
    "##################### DQN ##########################\n",
    "\n",
    "\n",
    "def to_one_hot(i, n_classes=None):\n",
    "    a = np.zeros(n_classes, 'uint8')\n",
    "    a[i] = 1\n",
    "    return a\n",
    "\n",
    "\n",
    "## Define Q-network q(a,s) that ouput the rewards of 4 actions by given state, i.e. Action-Value Function.\n",
    "# encoding for state: 4x4 grid can be represented by one-hot vector with 16 integers.\n",
    "def get_model(inputs_shape):\n",
    "    ni = tl.layers.Input(inputs_shape, name='observation')\n",
    "    nn = tl.layers.Dense(4, act=None, W_init=tf.random_uniform_initializer(0, 0.01), b_init=None, name='q_a_s')(ni)\n",
    "    return tl.models.Model(inputs=ni, outputs=nn, name=\"Q-Network\")\n",
    "\n",
    "\n",
    "def save_ckpt(model):  # save trained weights\n",
    "    path = os.path.join('model', '_'.join([alg_name, env_id]))\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    tl.files.save_weights_to_hdf5(os.path.join(path, 'dqn_model.hdf5'), model)\n",
    "\n",
    "\n",
    "def load_ckpt(model):  # load trained weights\n",
    "    path = os.path.join('model', '_'.join([alg_name, env_id]))\n",
    "    tl.files.save_weights_to_hdf5(os.path.join(path, 'dqn_model.hdf5'), model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    qnetwork = get_model([None, 16])\n",
    "    qnetwork.train()\n",
    "    train_weights = qnetwork.trainable_weights\n",
    "\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    t0 = time.time()\n",
    "    if args.train:\n",
    "        all_episode_reward = []\n",
    "        for i in range(num_episodes):\n",
    "            ## Reset environment and get first new observation\n",
    "            s = env.reset()  # observation is state, integer 0 ~ 15\n",
    "            rAll = 0\n",
    "            if render: env.render()\n",
    "            for j in range(99):  # step index, maximum step is 99\n",
    "                ## Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                allQ = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32)).numpy()\n",
    "                a = np.argmax(allQ, 1)\n",
    "\n",
    "                ## e-Greedy Exploration !!! sample random action\n",
    "                if np.random.rand(1) < e:\n",
    "                    a[0] = env.action_space.sample()\n",
    "                ## Get new state and reward from environment\n",
    "                s1, r, d, _ = env.step(a[0])\n",
    "                if render: env.render()\n",
    "                ## Obtain the Q' values by feeding the new state through our network\n",
    "                Q1 = qnetwork(np.asarray([to_one_hot(s1, 16)], dtype=np.float32)).numpy()\n",
    "\n",
    "                ## Obtain maxQ' and set our target value for chosen action.\n",
    "                maxQ1 = np.max(Q1)  # in Q-Learning, policy is greedy, so we use \"max\" to select the next action.\n",
    "                targetQ = allQ\n",
    "                targetQ[0, a[0]] = r + lambd * maxQ1\n",
    "                ## Train network using target and predicted Q values\n",
    "                # it is not real target Q value, it is just an estimation,\n",
    "                # but check the Q-Learning update formula:\n",
    "                #    Q'(s,a) <- Q(s,a) + alpha(r + lambd * maxQ(s',a') - Q(s, a))\n",
    "                # minimizing |r + lambd * maxQ(s',a') - Q(s, a)|^2 equals to force Q'(s,a) â‰ˆ Q(s,a)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    _qvalues = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32))\n",
    "                    _loss = tl.cost.mean_squared_error(targetQ, _qvalues, is_mean=False)\n",
    "                grad = tape.gradient(_loss, train_weights)\n",
    "                optimizer.apply_gradients(zip(grad, train_weights))\n",
    "\n",
    "                rAll += r\n",
    "                s = s1\n",
    "                ## Reduce chance of random action if an episode is done.\n",
    "                if d ==True:\n",
    "                    e = 1. / ((i / 50) + 10)  # reduce e, GLIE: Greey in the limit with infinite Exploration\n",
    "                    break\n",
    "\n",
    "            ## Note that, the rewards here with random action\n",
    "            print('Training  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}' \\\n",
    "                  .format(i, num_episodes, rAll, time.time() - t0))\n",
    "\n",
    "            if i == 0:\n",
    "                all_episode_reward.append(rAll)\n",
    "            else:\n",
    "                all_episode_reward.append(all_episode_reward[-1] * 0.9 + rAll * 0.1)\n",
    "\n",
    "        save_ckpt(qnetwork)  # save model\n",
    "        plt.plot(all_episode_reward)\n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "        plt.savefig(os.path.join('image', '_'.join([alg_name, env_id])))\n",
    "\n",
    "    if args.test:\n",
    "        load_ckpt(qnetwork)  # load model\n",
    "        for i in range(num_episodes):\n",
    "            ## Reset environment and get first new observation\n",
    "            s = env.reset()  # observation is state, integer 0 ~ 15\n",
    "            rAll = 0\n",
    "            if render: env.render()\n",
    "            for j in range(99):  # step index, maximum step is 99\n",
    "                ## Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                allQ = qnetwork(np.asarray([to_one_hot(s, 16)], dtype=np.float32)).numpy()\n",
    "                a = np.argmax(allQ, 1)  # no epsilon, only greedy for testing\n",
    "\n",
    "                ## Get new state and reward from environment\n",
    "                s1, r, d, _ = env.step(a[0])\n",
    "                rAll += r\n",
    "                s = s1\n",
    "                if render: env.render()\n",
    "                ## Reduce chance of random action if an episode is done.\n",
    "                if d: break\n",
    "\n",
    "            print('Testing  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}' \\\n",
    "                  .format(i, num_episodes, rAll, time.time() - t0))\n",
    "            rList.append(rAll)\n",
    "        print(\"Correct rate: \" + str(sum(rList) / num_episodes * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
