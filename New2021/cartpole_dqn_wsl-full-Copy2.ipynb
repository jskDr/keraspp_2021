{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 패키지\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 강화학습 환경 패키지\n",
    "import gym\n",
    "\n",
    "# 인공지능 패키지: 텐서플로, 케라스 \n",
    "# 호환성을 위해 텐스플로에 포함된 케라스를 불러옴 \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model(num_states, num_actions):\n",
    "    inputs = Input(shape=(num_states,))\n",
    "    layer = Dense(32, activation=\"relu\")(inputs)\n",
    "    layer = Dense(16, activation=\"relu\")(layer)\n",
    "    action = Dense(num_actions, activation=\"linear\")(layer)\n",
    "    return Model(inputs=inputs, outputs=action)\n",
    "\n",
    "model = create_q_model(4,2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    state_size = 4\n",
    "    action_size = 2        \n",
    "    states = np.zeros((10,state_size), dtype=np.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicts = model(states)\n",
    "\n",
    "def get_env_model():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    num_states = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    model = create_q_model(num_states, num_actions)\n",
    "    return env, model\n",
    "\n",
    "env, model = get_env_model()\n",
    "train(model)\n",
    "print('Simple training is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_00:\n",
    "    def __init__(self):\n",
    "        self.get_env_model()\n",
    "\n",
    "    def get_env_model(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.model = create_q_model(self.num_states, self.num_actions)\n",
    "        # print(self.model.summary())\n",
    "\n",
    "    def train(self):        \n",
    "        states = np.zeros((10,self.num_states), dtype=np.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicts = self.model(states)\n",
    "\n",
    "new_world = World_00()\n",
    "new_world.train()\n",
    "print('Simple training is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_test_model_memory(memory, env, model, n_episodes=1000, \n",
    "        flag_render=False):\n",
    "    for e in range(n_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            s_array = np.array(s).reshape((1,-1))\n",
    "            Qsa = model.predict(s_array)[0]\n",
    "            a = np.argmax(Qsa)\n",
    "            next_s, r, done, _ = env.step(a)\n",
    "            if flag_render:\n",
    "                env.render()\n",
    "            score += r\n",
    "            memory.append([s,a,r,next_s,done])\n",
    "        print(f'Episode: {e:5d} -->  Score: {score:3.1f}')\n",
    "    print('Notice that the max score is set to 500.0 in CartPole-v1')\n",
    "\n",
    "def list_rotate(l):\n",
    "    return list(zip(*l))\n",
    "\n",
    "class World_01(World_00):\n",
    "    def __init__(self):\n",
    "        World_00.__init__(self)\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.N_batch = 64\n",
    "        self.t_model = create_q_model(self.num_states, self.num_actions)\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "\n",
    "    def trial(self, flag_render=False):\n",
    "        env_test_model_memory(self.memory, self.env,\n",
    "            self.model, n_episodes=10, flag_render=flag_render)\n",
    "        print(len(self.memory))\n",
    "\n",
    "    def train_memory(self):\n",
    "        if len(self.memory) >= self.N_batch:\n",
    "            memory_batch = random.sample(self.memory, self.N_batch)\n",
    "            s_l,a_l,r_l,next_s_l,done_l = [np.array(x) for x in list_rotate(memory_batch)]\n",
    "            model_w = self.model.trainable_variables\n",
    "            with tf.GradientTape() as tape:\n",
    "                Qsa_pred_l = self.model(s_l.astype(np.float32))\n",
    "                a_l_onehot = tf.one_hot(a_l, self.num_actions)\n",
    "                Qs_a_pred_l = tf.reduce_sum(a_l_onehot * Qsa_pred_l, axis=1)    \n",
    "\n",
    "                Qsa_tpred_l = self.t_model(next_s_l.astype(np.float32)) \n",
    "                Qsa_tpred_l = tf.stop_gradient(Qsa_tpred_l)\n",
    "\n",
    "                max_Q_next_s_a_l = np.amax(Qsa_tpred_l, axis=-1)\n",
    "                Qs_a_l = r_l + (1 - done_l) * self.discount_factor * max_Q_next_s_a_l\n",
    "                loss = tf.reduce_mean(tf.square(Qs_a_l - Qs_a_pred_l))\n",
    "            grads = tape.gradient(loss, model_w)\n",
    "            self.optimizer.apply_gradients(zip(grads, model_w))\n",
    "\n",
    "new_world = World_01()\n",
    "new_world.trial()\n",
    "new_world.train_memory()\n",
    "new_world.env.close()\n",
    "print('Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_02(World_01):\n",
    "    def __init__(self):\n",
    "        World_01.__init__(self)\n",
    "        self.epsilon = 0.2\n",
    "    \n",
    "    def update_t_model(self):\n",
    "        self.t_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def best_action(self, s):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            s_array = np.array(s).reshape((1,-1))\n",
    "            Qsa = self.model.predict(s_array)[0]\n",
    "            return np.argmax(Qsa)\n",
    "\n",
    "    def trials(self, n_episodes=100, flag_render=False):\n",
    "        memory = self.memory\n",
    "        env = self.env\n",
    "        model = self.model\n",
    "        score_l = []\n",
    "        for e in range(n_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            s = env.reset()\n",
    "            while not done:                \n",
    "                a = self.best_action(s)\n",
    "                next_s, r, done, _ = env.step(a)\n",
    "                if flag_render:\n",
    "                    env.render()\n",
    "                score += r\n",
    "                memory.append([s,a,r,next_s,done])\n",
    "                # self.train_memory()     \n",
    "                s = next_s\n",
    "                self.train_memory()                 \n",
    "            self.update_t_model()\n",
    "            print(f'Episode: {e:5d} -->  Score: {score:3.1f}') \n",
    "            score_l.append(score)            \n",
    "        return score_l\n",
    "\n",
    "new_world = World_02()\n",
    "score_l = new_world.trials(n_episodes=50)\n",
    "new_world.env.close()\n",
    "np.save('score_l.npy', score_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6efac23f2bcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mnew_world\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorldFull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mscore_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mnew_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6efac23f2bcc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWorldFull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_env_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6efac23f2bcc>\u001b[0m in \u001b[0;36mget_env_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_env_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_q_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 기본 패키지\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 강화학습 환경 패키지\n",
    "import gym\n",
    "\n",
    "# 인공지능 패키지: 텐서플로, 케라스 \n",
    "# 호환성을 위해 텐스플로에 포함된 케라스를 불러옴 \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_q_model(num_states, num_actions):\n",
    "    inputs = Input(shape=(num_states,))\n",
    "    layer = Dense(32, activation=\"relu\")(inputs)\n",
    "    layer = Dense(16, activation=\"relu\")(layer)\n",
    "    action = Dense(num_actions, activation=\"linear\")(layer)\n",
    "    return Model(inputs=inputs, outputs=action)\n",
    "\n",
    "class WorldFull():\n",
    "    def __init__(self):\n",
    "        self.get_env_model() #? \n",
    "        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.N_batch = 64\n",
    "        self.t_model = create_q_model(self.num_states, self.num_actions)\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "        \n",
    "        self.epsilon = 0.2\n",
    "        \n",
    "    def get_env_model(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.model = create_q_model(self.num_states, self.num_actions)\n",
    "    \n",
    "    def update_t_model(self):\n",
    "        self.t_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def best_action(self, s):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            s_array = np.array(s).reshape((1,-1))\n",
    "            Qsa = self.model.predict(s_array)[0]\n",
    "            return np.argmax(Qsa)\n",
    "\n",
    "    def trials(self, n_episodes=100, flag_render=False):\n",
    "        memory = self.memory\n",
    "        env = self.env\n",
    "        model = self.model\n",
    "        score_l = []\n",
    "        for e in range(n_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            s = env.reset()\n",
    "            while not done:                \n",
    "                a = self.best_action(s)\n",
    "                next_s, r, done, _ = env.step(a)\n",
    "                if flag_render:\n",
    "                    env.render()\n",
    "                score += r\n",
    "                memory.append([s,a,r,next_s,done])\n",
    "                # self.train_memory()     \n",
    "                s = next_s\n",
    "                self.train_memory()                 \n",
    "            self.update_t_model()\n",
    "            print(f'Episode: {e:5d} -->  Score: {score:3.1f}') \n",
    "            score_l.append(score)            \n",
    "        return score_l\n",
    "\n",
    "new_world = WorldFull()\n",
    "score_l = new_world.trials(n_episodes=5)\n",
    "new_world.env.close()\n",
    "np.save('score_l.npy', score_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
